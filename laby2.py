# -*- coding: utf-8 -*-
"""laby2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hUEhSwHwTnHv-7L4ov2-Llram0DM5fD1
"""

import numpy as np
import cupy as cp
import time

# Rozmiar macierzy
N = 1024

# --- Mnożenie na CPU ---
A_cpu = np.random.rand(N, N).astype(np.float32)
B_cpu = np.random.rand(N, N).astype(np.float32)

start = time.time()
C_cpu = np.matmul(A_cpu, B_cpu)
end = time.time()
print(f"Mnożenie macierzy na CPU trwało: {end - start:.5f} s")

# Wyświetlenie macierzy A, B i wyniku C na CPU
print("\nMacierz A (CPU):")
print(A_cpu)
print("\nMacierz B (CPU):")
print(B_cpu)
print("\nWynik mnożenia na CPU (C):")
print(C_cpu)

# --- Mnożenie na GPU (CuPy) ---
A_gpu = cp.array(A_cpu)
B_gpu = cp.array(B_cpu)

start = time.time()
C_gpu = cp.matmul(A_gpu, B_gpu)
cp.cuda.Stream.null.synchronize()
end = time.time()
print(f"Mnożenie macierzy na GPU trwało: {end - start:.5f} s")

# Wyświetlenie wyniku na GPU
C_check = cp.asnumpy(C_gpu)
print("\nWynik mnożenia na GPU (C):")
print(C_check)

# --- Weryfikacja poprawności ---
print("\nCzy wyniki są takie same na CPU i GPU:", np.allclose(C_cpu, C_check))

# Zadanie 2: Implementacja kernela CUDA w PyCUDA

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!pip install pycuda

import numpy as np
import pycuda.driver as cuda
import pycuda.autoinit
from pycuda.compiler import SourceModule
import time

# --- Kernel CUDA ---
kernel_code = """
__global__ void matrixMul(float *A, float *B, float *C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float sum = 0.0;
    if (row < N && col < N) {
        for (int k = 0; k < N; k++) {
            sum += A[row * N + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}
"""

# --- Kompilacja kernela ---
mod = SourceModule(kernel_code)
matrixMul = mod.get_function("matrixMul")

# --- Dane wejściowe ---
N = 1024
A_cpu = np.random.rand(N, N).astype(np.float32)
B_cpu = np.random.rand(N, N).astype(np.float32)
C_cpu = np.empty((N, N), np.float32)

# --- Alokacja pamięci na GPU ---
A_gpu = cuda.mem_alloc(A_cpu.nbytes)
B_gpu = cuda.mem_alloc(B_cpu.nbytes)
C_gpu = cuda.mem_alloc(C_cpu.nbytes)

# --- Przesłanie danych ---
cuda.memcpy_htod(A_gpu, A_cpu)
cuda.memcpy_htod(B_gpu, B_cpu)

# --- Parametry siatki i bloku ---
block_size = (32, 32, 1)
grid_size = (int(N / 32), int(N / 32), 1)

# --- Uruchomienie kernela ---
start = time.time()
matrixMul(A_gpu, B_gpu, C_gpu, np.int32(N), block=block_size, grid=grid_size)
cuda.Context.synchronize()
end = time.time()

# --- Pobranie wyników ---
cuda.memcpy_dtoh(C_cpu, C_gpu)

print(f"Mnożenie macierzy na GPU (PyCUDA) trwało: {end - start:.5f} s")

# --- Czyszczenie pamięci ---
del A_gpu, B_gpu, C_gpu

# Zadanie 3: Analiza i pomiar efektywności dla różnych rozmiarów macierzy

import numpy as np
import cupy as cp
import matplotlib.pyplot as plt
import time

sizes = [128, 256, 512, 1024, 2048]
cpu_times = []
gpu_times = []

for N in sizes:
    A_cpu = np.random.rand(N, N).astype(np.float32)
    B_cpu = np.random.rand(N, N).astype(np.float32)

    # --- CPU ---
    start = time.time()
    np.matmul(A_cpu, B_cpu)
    cpu_times.append(time.time() - start)

    # --- GPU ---
    A_gpu = cp.array(A_cpu)
    B_gpu = cp.array(B_cpu)
    start = time.time()
    cp.matmul(A_gpu, B_gpu)
    cp.cuda.Stream.null.synchronize()
    gpu_times.append(time.time() - start)

# --- Wykres ---
plt.figure(figsize=(8, 5))
plt.plot(sizes, cpu_times, 'o-', label='CPU (NumPy)')
plt.plot(sizes, gpu_times, 's-', label='GPU (CuPy)')
plt.xlabel("Rozmiar macierzy N")
plt.ylabel("Czas [s]")
plt.title("Porównanie efektywności CPU vs GPU")
plt.legend()
plt.grid(True)
plt.show()

# Lista rozmiarów macierzy do porównania
sizes = [128, 256, 512, 1024, 2048]
cpu_times = []
gpu_times = []

# --- Kernel CUDA ---
kernel_code = """
__global__ void matrixMul(float *A, float *B, float *C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float sum = 0.0;
    if (row < N && col < N) {
        for (int k = 0; k < N; k++) {
            sum += A[row * N + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}
"""

# Kompilacja kernela
mod = SourceModule(kernel_code)
matrixMul = mod.get_function("matrixMul")

# Przechodzimy przez różne rozmiary macierzy
for N in sizes:
    # Generowanie macierzy
    A_cpu = np.random.rand(N, N).astype(np.float32)
    B_cpu = np.random.rand(N, N).astype(np.float32)
    C_cpu = np.empty((N, N), np.float32)

    # --- Mnożenie na CPU ---
    start = time.time()
    np.matmul(A_cpu, B_cpu)
    cpu_times.append(time.time() - start)

    # --- Mnożenie na GPU (PyCUDA) ---
    A_gpu = cuda.mem_alloc(A_cpu.nbytes)
    B_gpu = cuda.mem_alloc(B_cpu.nbytes)
    C_gpu = cuda.mem_alloc(C_cpu.nbytes)

    cuda.memcpy_htod(A_gpu, A_cpu)
    cuda.memcpy_htod(B_gpu, B_cpu)

    block_size = (32, 32, 1)
    grid_size = (int(N / 32), int(N / 32), 1)

    start = time.time()
    matrixMul(A_gpu, B_gpu, C_gpu, np.int32(N), block=block_size, grid=grid_size)
    cuda.Context.synchronize()

    cuda.memcpy_dtoh(C_cpu, C_gpu)
    gpu_times.append(time.time() - start)

    # --- Zwalnianie pamięci GPU ---
    del A_gpu, B_gpu, C_gpu

# --- Wykres porównania czasów CPU vs GPU ---
plt.figure(figsize=(8, 5))
plt.plot(sizes, cpu_times, 'o-', label='CPU (NumPy)', color='blue')
plt.plot(sizes, gpu_times, 's-', label='GPU (PyCUDA)', color='green')
plt.xlabel("Rozmiar macierzy (N)")
plt.ylabel("Czas (sekundy)")
plt.title("Porównanie wydajności CPU vs GPU (PyCUDA) przy mnożeniu macierzy")
plt.legend()
plt.grid(True)
plt.show()